import pandas as pd
import numpy as np
import openai
from openai import OpenAI
import os
from pathlib import Path
from tqdm import tqdm
from dotenv import load_dotenv
import json
import tiktoken  # for token counting
from collections import defaultdict
from decimal import Decimal
import matplotlib.pyplot as plt

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)


def generate_helpful_probability_original_note(dataset):
    for index, row in tqdm(dataset.iterrows()):
        tweet = row['tweet_text']
        note = row['original_note']
        prompt = f"I'm going to show you a Tweet and a Note about the tweet. Notes are supposed to clarify potential misinformation present in the Tweet. A helpful Note should be accurate and important. I'd like you to rate whether or not the Note is helpful. Respond with one of 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8 or 0.9. 0.1 means it is not helpful at all and 0.9 means it is the most helpful. Here is the Tweet: {tweet}. Here is the Note: {note}. Give a probability of how helpful this note is and nothing else."

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=4
        )
        dataset.at[index, 'original_note_helpfulness_probability'] = response.choices[0].message.content
    return dataset


df = pd.read_csv('master.csv')
num_rows = df.shape[0]
#result_df = generate_helpful_probability_original_note(df)
#result_df.to_csv('master.csv', index=False)
buckets = [[] for _ in range(9)]
for index, row in df.iterrows(): #iterates through csv and allocates probabilities generated by GPT and the helpfulness status labels of the note to their matching bucket 
    helpfulness_probability = row['original_note_helpfulness_probability']
    current_status = row['currentStatus']
    if helpfulness_probability == '0.1': 
        buckets[0].append((current_status,helpfulness_probability))
    elif helpfulness_probability == '0.2':
         buckets[1].append((current_status,helpfulness_probability))
    elif helpfulness_probability == '0.3':
         buckets[2].append((current_status,helpfulness_probability))
    elif helpfulness_probability == '0.4':
         buckets[3].append((current_status,helpfulness_probability))
    elif helpfulness_probability == '0.5':
         buckets[4].append((current_status,helpfulness_probability))
    elif helpfulness_probability == '0.6':
         buckets[5].append((current_status,helpfulness_probability))
    elif helpfulness_probability == '0.7':
         buckets[6].append((current_status,helpfulness_probability))
    elif helpfulness_probability == '0.8':
         buckets[7].append((current_status,helpfulness_probability))
    elif helpfulness_probability == '0.9': 
        buckets[8].append((current_status,helpfulness_probability))


ece = 0
mean_predicted_probabilities = np.array([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])
fraction_of_positives = []

for bucket in buckets:
    bucket_accuracy = 0
    bucket_trues = 0
    bucket_confidence = 0
    bucket_ece = 0
    for true_label,predicted_label in bucket:
       bucket_confidence += Decimal(predicted_label)
       if true_label == 'CURRENTLY_RATED_HELPFUL':
        bucket_trues += 1 #keeps track of how many notes are rated helpful per bucket

    
   
    bucket_accuracy = Decimal(bucket_trues) / Decimal(len(bucket)) # this decimal indicates how many notes were rated helpful per bucket
    fraction_of_positives.append(bucket_accuracy)
    bucket_confidence /= Decimal(len(bucket))
    bucket_ece = (Decimal(len(bucket))/ num_rows) * abs(bucket_accuracy - bucket_confidence)
    ece += bucket_ece
print("Expected Calibration Error:", ece) # Expected Calibration Error: 0.1835616438356164383561643835


error_sum = 0
for index, row in df.iterrows():
    predicted_label = row['original_note_helpfulness_probability']
    actual_label = row['currentStatus']
    
    try:
     predicted_float = float(predicted_label)
     if actual_label == 'CURRENTLY_RATED_HELPFUL':
        error_sum += pow((Decimal(predicted_label) - Decimal('1')), 2)
     elif actual_label == 'CURRENTLY_RATED_NOT_HELPFUL':
        error_sum += pow((Decimal(predicted_label) - Decimal('0')), 2)
     else:
        error_sum += 0
    except ValueError:
     pass


brier_score = error_sum / num_rows
print("Brier Score: ", brier_score) # Brier Score:  0.2311643835616438356164383562

fraction_of_positives = np.array(fraction_of_positives)
fraction_of_positives = [float(value) for value in fraction_of_positives]

# Plot the graph
plt.plot(mean_predicted_probabilities, fraction_of_positives, marker='o', linestyle='--', color='b')
plt.xlabel('Predicted probability of note is helpful')
plt.ylabel('Percent of notes that were actually rated by humans')
plt.title('Graph between Mean Predicted Probability and Percent of notes that were actually rated by humans')
plt.show()



  



        
          
      


